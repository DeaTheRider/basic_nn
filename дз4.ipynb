{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PYMmt00rOFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "c38ee450-4052-483a-e4bd-a1638ce8d47d"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras # расскоментируйте эту строку, чтобы начать обучение\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "# установка параметров нейросети\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "data_augmentation = False\n",
        "num_predictions = 20\n",
        "#save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "#model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# разделение тренировочной и тестовой выборки\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'тренировочные примеры')\n",
        "print(x_test.shape[0], 'тестовые примеры')\n",
        "\n",
        "# преобразование матрицы чисел 0-9 в бинарную матрицу чисел 0-1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# конфигурирование слоев нейросети\n",
        "model = Sequential()\n",
        "\n",
        "# слои нейросети отвественные за свертку и max-pooling\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# полносвязные слои нейронной сети\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# инициализация RMSprop optimizer\n",
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "# компиляция модели\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Не используется data augmentation')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Использование data augmentation в реальном времени')\n",
        "    # Препроцессинг и data augmentation в реальном времени:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        samplewise_center=True,\n",
        "        featurewise_std_normalization=False,\n",
        "        samplewise_std_normalization=False,\n",
        "        zca_whitening=False, \n",
        "        zca_epsilon=1e-06, \n",
        "        rotation_range=0, \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0., \n",
        "        zoom_range=0., \n",
        "        channel_shift_range=0.,\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        rescale=None,\n",
        "        preprocessing_function=None,\n",
        "        data_format=None,\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # запуск data augmentation через fit\n",
        "    #datagen.fit(x_train)\n",
        "\n",
        "    # запуск data augmentation через fit_generator\n",
        "    model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "# сохранение модели и весов\n",
        "# if not os.path.isdir(save_dir):\n",
        "#     os.makedirs(save_dir)\n",
        "# model_path = os.path.join(save_dir, model_name)\n",
        "# model.save(model_path)\n",
        "# print('сохранить обученную модель как %s ' % model_path)\n",
        "\n",
        "# проверка работы обученной модели\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 тренировочные примеры\n",
            "10000 тестовые примеры\n",
            "Не используется data augmentation\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 9s 187us/step - loss: 1.6035 - accuracy: 0.4093 - val_loss: 1.2876 - val_accuracy: 0.5443\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 1.2734 - accuracy: 0.5422 - val_loss: 1.1080 - val_accuracy: 0.6104\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 9s 179us/step - loss: 1.1192 - accuracy: 0.6031 - val_loss: 0.9598 - val_accuracy: 0.6647\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 1.0194 - accuracy: 0.6407 - val_loss: 0.9101 - val_accuracy: 0.6811\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 0.9497 - accuracy: 0.6647 - val_loss: 0.7966 - val_accuracy: 0.7263\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.9091 - accuracy: 0.6796 - val_loss: 0.7911 - val_accuracy: 0.7255\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 0.8766 - accuracy: 0.6935 - val_loss: 0.7725 - val_accuracy: 0.7321\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 9s 182us/step - loss: 0.8396 - accuracy: 0.7039 - val_loss: 0.7391 - val_accuracy: 0.7421\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 0.8233 - accuracy: 0.7123 - val_loss: 0.7437 - val_accuracy: 0.7402\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.8011 - accuracy: 0.7186 - val_loss: 0.7473 - val_accuracy: 0.7390\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.7830 - accuracy: 0.7253 - val_loss: 0.7000 - val_accuracy: 0.7587\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 9s 182us/step - loss: 0.7625 - accuracy: 0.7326 - val_loss: 0.7213 - val_accuracy: 0.7493\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 9s 183us/step - loss: 0.7533 - accuracy: 0.7359 - val_loss: 0.6565 - val_accuracy: 0.7733\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 9s 183us/step - loss: 0.7448 - accuracy: 0.7357 - val_loss: 0.6721 - val_accuracy: 0.7645\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 9s 185us/step - loss: 0.7293 - accuracy: 0.7437 - val_loss: 0.6738 - val_accuracy: 0.7642\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.7108 - accuracy: 0.7522 - val_loss: 0.6500 - val_accuracy: 0.7733\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 9s 179us/step - loss: 0.7122 - accuracy: 0.7500 - val_loss: 0.6389 - val_accuracy: 0.7781\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.7016 - accuracy: 0.7519 - val_loss: 0.6719 - val_accuracy: 0.7699\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 9s 180us/step - loss: 0.6922 - accuracy: 0.7559 - val_loss: 0.6750 - val_accuracy: 0.7648\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 9s 181us/step - loss: 0.6802 - accuracy: 0.7616 - val_loss: 0.6325 - val_accuracy: 0.7783\n",
            "10000/10000 [==============================] - 1s 99us/step\n",
            "Test loss: 0.6324885167121888\n",
            "Test accuracy: 0.7782999873161316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQdY5L6NxpJH",
        "colab_type": "text"
      },
      "source": [
        "Из за того что картинки маленькие аугментация не помогает . Для увелечения скоро помогло изменение оптимизатора , увелечение эпох и изменение дроп аута до половины. Для подгона к cifar100 нужно изменить голову , а для imagenet необходимо подогнать изображение по размеру\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0XQ2Tror1gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}