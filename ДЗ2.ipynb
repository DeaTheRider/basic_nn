{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFGNMGckOLW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7323c5f6-c9fa-4498-eb9f-038202e69533"
      },
      "source": [
        "import numpy as np\n",
        "import mnist\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "print(train_images.shape) # (60000, 784)\n",
        "print(test_images.shape)  # (10000, 784)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJv3_Wz8OSMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XarizKdkVE1q",
        "colab_type": "text"
      },
      "source": [
        "В увелечение скора помогло смена оптимизатора ,  дроп аут и увеличение размера батча. Посколько маленький датасет , можно увеличить колличество эпох чтобы проскочить локальный минимум , так как все оптимизаторы являются стохастическими \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj9VQse7ONYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "b7cf72d2-dbc5-491e-cc09-f56b5b7f100b"
      },
      "source": [
        "# The full neural network code!\n",
        "###############################\n",
        "import numpy as np\n",
        "import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "np.random.seed(42)\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  #Dropout(0.6),\n",
        "  BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None),\n",
        "  Dense(64, activation='relu'),\n",
        "  #Dropout(0.6),\n",
        "  BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None),\n",
        "  Dense(32, activation='relu'),\n",
        "  #Dropout(0.6),\n",
        "  BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='Nadam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=15,\n",
        "  batch_size=256,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "#model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.3472 - accuracy: 0.9013\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.1342 - accuracy: 0.9593\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.1004 - accuracy: 0.9697\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0811 - accuracy: 0.9747\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0687 - accuracy: 0.9785\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0602 - accuracy: 0.9805\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0535 - accuracy: 0.9824\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0466 - accuracy: 0.9847\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0433 - accuracy: 0.9863\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0384 - accuracy: 0.9873\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0354 - accuracy: 0.9882\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0342 - accuracy: 0.9887\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0317 - accuracy: 0.9893\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0277 - accuracy: 0.9906\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0260 - accuracy: 0.9911\n",
            "10000/10000 [==============================] - 0s 33us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBMrN0G5OPDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3d252e3c-c9f8-4cac-f157-ecae16482a97"
      },
      "source": [
        "# The full neural network code!\n",
        "###############################\n",
        "import numpy as np\n",
        "import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=5,\n",
        "  batch_size=32,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.3725 - accuracy: 0.8881\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1955 - accuracy: 0.9415\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1528 - accuracy: 0.9535\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1316 - accuracy: 0.9594\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1130 - accuracy: 0.9655\n",
            "10000/10000 [==============================] - 0s 25us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd2h2m55P-r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}